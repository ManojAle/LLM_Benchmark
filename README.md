# Abstract
Large Language Models showed significant potential in the Code Generation. However, Consistancy of the code is not reliable enough in General Code Generation. On top of that, Performance related code optimization is very important while Generating code. our Objective is to analyze our PerfCurator Performance related codes to setup LLM benchmarking for various individual tasks. An Ideal Model should be generating the performance level code on its own with the descriptions. those descriptions can be preserved to high level performance code on its own. any lack of in-consistancy while generating code or generating descriptions will create a in-consistancy. so we plan to evaluate LLMs self consistancy when they do tasks Programming language to Natural language and Natural language to programming language with focus of performance related code data. Our Focus is to Explain the self consistancy of the LLMs on performance level data in the PL to NL and NL to PL. then we designed a framework to create this self consistancy with our PerfCurator performance data to evaluate the LLMs. Our study is to consider the top Code Generation models performance check with our LLM benchmarking. 

## Introduction

For Evaluating Code LLMs people using Existing benchmaking CodeXBleu, HumanEval and so on. But for Evaluating Performance level Generation of the code using LLM is yet to be Explored. It requires high quality performance data to set it up robust Evaluation design. Also Conventional LLM individual tasks with LLMs is Primarily focus on the 

1. Code Generation: we Generate Code Generation from the clear description. it is a NL to PL task.
2. Code Summarization: we generate Summary for the code. it is PL to NL task.

Our Primary objective is to show the self consistancy of the LLMs using our Performance data. Our performance data is mined from Git Repository. these data is crafted by various authors in the Git repositories. Our data will have Code before, Code After, Commit message, Category of the performance issue and patch informations. we collected a lot of high level performance code and categorized them into APIMisuse, MemoryInefficiency, PoorConcurrencyControl, InefficientI/O, 
 NetworkBottlenecks, InefficientAlgorithm/Data-structure, Parallelization, Micro-architectural. so we want to evaluate the LLMs on these categories. Our design for the Experimentation is we want to check the LLM how it preserves to Generate the Source code from it's own description. we start with code summarization using a top 4 LLMs in the Hugggingface Code LLMs leaderboard. we have used CodeInterpreter, Artigenz Coder, CodeGwen, NXCoder. we provide code before,code after and commit message as a input. we craft a prompt to focus on the Performance in-efficiency in the code and how they fixed it. we get quality description which talks about the performance issue and how it is optimized. for Next step our Code Generation task ( NL to PL) we provide code before + descritoption to optimize the issue in the code before. finally, we augument the code before and after with the help of LLM to executable then we execute them to check the performance. For Augumenting the inputs as a executable code,  we use GPT-4o. we Evaluate the Augumented code Manually to get High quality data points. which will be used for Executable Experiment.

so, Executable Experiment, we run the code for N iterations 5 times to make sure our code before run for more than 5s. 
